# -*- coding: utf-8 -*-
"""RAG_Prortotype.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_aI0I7YqyqOywmcfJs4GxksjcQqzsY6K
"""

!pip install llama-index[full] PyPDF2 openai

from google.colab import files
uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]

from llama_index.readers.file import PDFReader

reader = PDFReader()
documents = reader.load_data(file=pdf_path)

from llama_index.core.node_parser import SimpleNodeParser

parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=50)
nodes = parser.get_nodes_from_documents(documents)

print(f"Total chunks created: {len(nodes)}")
print(nodes[0].get_content())  # View first chunk

# Step 1: Load your PDF document
from llama_index.readers.file import PDFReader

reader = PDFReader()
documents = reader.load_data('Graphical_Lasso.pdf')  # <- Replace with your actual file name

# Step 2: Parse into nodes
from llama_index.core.node_parser import SimpleNodeParser

parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=50)
nodes = parser.get_nodes_from_documents(documents)

# Step 3: Use HuggingFace embedding and create index
from llama_index.core import VectorStoreIndex
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
index = VectorStoreIndex(nodes, embed_model=embed_model)

from llama_index.core import Settings, VectorStoreIndex
from llama_index.core.query_engine import RetrieverQueryEngine

# Set the embedding model (you already created this earlier)
Settings.embed_model = embed_model

# Disable LLM
Settings.llm = None

# Now create index (if not already)
index = VectorStoreIndex(nodes)

# Create query engine
query_engine = index.as_query_engine()

# Run a query
response = query_engine.query("Find a definition or explanation of the graphical lasso")
print(response)

!pip install llama-index-embeddings-huggingface

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import VectorStoreIndex

embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en")
index = VectorStoreIndex(nodes, embed_model=embed_model)

# from llama_index.core import VectorStoreIndex
# from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# # Set up a free local embedding model from Hugging Face
# embed_model = HuggingFaceEmbedding(model_name="all-MiniLM-L6-v2")  # small & fast

# # Create vector index from parsed nodes
# index = VectorStoreIndex(nodes, embed_model=embed_model)

from fastapi import FastAPI
from pydantic import BaseModel

# Sample RAG setup â€” replace with your actual query_engine
from llama_index.core import VectorStoreIndex
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

index = VectorStoreIndex.load_from_disk("index.json")  # or recreate from nodes
query_engine = index.as_query_engine()

app = FastAPI()

class QueryRequest(BaseModel):
    question: str

@app.post("/query")
def get_answer(req: QueryRequest):
    response = query_engine.query(req.question)
    return {"answer": str(response)}